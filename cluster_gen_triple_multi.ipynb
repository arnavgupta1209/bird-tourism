{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dd = dd.read_csv(\"india_birds\\ebd_IN_smp_relMar-2024_sampling.txt\", sep='\\t', header=0, dtype=object)\n",
    "columns = data_dd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_geojson = gpd.read_file(\"india-composite.geojson\")\n",
    "india_geojson = india_geojson.to_crs(\"WGS84\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter only Traveling and Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dd = data_dd[[\"STATE\", \"LATITUDE\", \"LONGITUDE\", \"OBSERVATION DATE\", \"OBSERVER ID\", \"PROTOCOL TYPE\",'TIME OBSERVATIONS STARTED','SAMPLING EVENT IDENTIFIER', 'PROJECT CODE', 'GROUP IDENTIFIER']]\n",
    "data_dd = data_dd[data_dd[\"PROTOCOL TYPE\"].isin([\"Traveling\", \"Stationary\"])]\n",
    "data_dd = data_dd.drop(\"PROTOCOL TYPE\", axis=1)\n",
    "data_dd[\"GROUP IDENTIFIER\"] = data_dd[\"GROUP IDENTIFIER\"].fillna(\"No Group\")\n",
    "data_dd = data_dd.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_dd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"TIME OBSERVATIONS STARTED\"] = pd.to_datetime(data_df[\"TIME OBSERVATIONS STARTED\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dd.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"LATITUDE\"] = data_df[\"LATITUDE\"].astype(float)\n",
    "data_df[\"LONGITUDE\"] = data_df[\"LONGITUDE\"].astype(float)\n",
    "data_df[\"OBSERVATION DATE\"] = pd.to_datetime(data_df[\"OBSERVATION DATE\"])\n",
    "data_df[\"STATE\"] = data_df[\"STATE\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"DATE TIME\"] = data_df[\"OBSERVATION DATE\"].astype(str) + \" \" + data_df[\"TIME OBSERVATIONS STARTED\"].dt.time.astype(str)\n",
    "data_df[\"DATE TIME\"] = pd.to_datetime(data_df[\"DATE TIME\"], errors='coerce')\n",
    "data_df[\"TIME OBSERVATIONS STARTED\"] = data_df[\"TIME OBSERVATIONS STARTED\"].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numlist_df = data_df.groupby(\"OBSERVER ID\").size().reset_index(name='NUMLIST')\n",
    "numlist_df_filtered = numlist_df[numlist_df[\"NUMLIST\"] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered = data_df[data_df[\"OBSERVER ID\"].isin(numlist_df_filtered[\"OBSERVER ID\"])]\n",
    "data_df_filtered = data_df_filtered.merge(numlist_df_filtered, on=\"OBSERVER ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "X = data_df_filtered[[\"LATITUDE\", \"LONGITUDE\"]]\n",
    "#bandwidth = estimate_bandwidth(X, quantile=0.01, n_samples=10000, n_jobs=-1)\n",
    "#bandwidth = estimate_bandwidth(X, quantile=0.005, n_samples=10000, n_jobs=-1)\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.5, n_samples=20000, n_jobs=-1, random_state=0)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True, n_jobs=-1)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "data_df_filtered[\"CLUSTER\"] = labels\n",
    "data_df_filtered[\"CLUSTER\"] = data_df_filtered[\"CLUSTER\"].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_first = cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_map = data_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 20000\n",
    "fig = px.scatter_mapbox(first_map.sample(sampling_rate), lat=\"LATITUDE\", lon=\"LONGITUDE\", color=\"CLUSTER\", zoom=2)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\", width=1000, height=1000)\n",
    "fig.update_layout(mapbox={\"layers\":[\n",
    "            {\n",
    "                \"source\": india_geojson[\"geometry\"].__geo_interface__,\n",
    "                \"type\": \"line\",\n",
    "                \"color\": \"black\",\n",
    "                \"line\": {\"width\": 0.5},\n",
    "            }\n",
    "        ]})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "second_clusters = []\n",
    "second_cluster_centers = []\n",
    "for i in range(len(cluster_centers_first)):\n",
    "    X = data_df_filtered[data_df_filtered[\"CLUSTER\"] == i][[\"LATITUDE\", \"LONGITUDE\"]]\n",
    "    #bandwidth = estimate_bandwidth(X, quantile=0.01, n_samples=10000, n_jobs=-1)\n",
    "    #bandwidth = estimate_bandwidth(X, quantile=0.005, n_samples=10000, n_jobs=-1)\n",
    "    #bandwidth = estimate_bandwidth(X, quantile=0.017, n_samples=20000, n_jobs=-1, random_state=0)\n",
    "    bandwidth = estimate_bandwidth(X, quantile=0.014, n_samples=20000, n_jobs=-1, random_state=0)\n",
    "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True, n_jobs=-1)\n",
    "    ms.fit(X)\n",
    "    labels = ms.labels_\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "    second_clusters.append(labels)\n",
    "    second_cluster_centers.append(cluster_centers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_clusters_indexes = []\n",
    "\n",
    "for i in range(len(second_clusters)):\n",
    "    for j in range(len(np.unique(second_clusters[i]))):\n",
    "        second_clusters_indexes.append((i+1)*1000+j)\n",
    "\n",
    "second_clusters_indexes = np.array(second_clusters_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_cluster_centers = np.concatenate(second_cluster_centers, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_clusters_df = pd.DataFrame(second_cluster_centers, columns=[\"LATITUDE\", \"LONGITUDE\"])\n",
    "second_clusters_df[\"CLUSTER\"] = second_clusters_indexes\n",
    "second_clusters_df[\"CLUSTER\"] = second_clusters_df[\"CLUSTER\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cluster_centers_first)):\n",
    "    data_df_filtered.loc[data_df_filtered[\"CLUSTER\"] == i, \"SECOND CLUSTER\"] = second_clusters[i]+((i+1)*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered[\"SECOND CLUSTER cat\"] = data_df_filtered[\"SECOND CLUSTER\"].astype('int64').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_map = data_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 40000\n",
    "fig = px.scatter_mapbox(second_map.sample(sampling_rate, replace=True), lat=\"LATITUDE\", lon=\"LONGITUDE\", color=\"SECOND CLUSTER cat\", zoom=2)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\", width=1000, height=1000)\n",
    "fig.update_layout(mapbox={\"layers\":[\n",
    "            {\n",
    "                \"source\": india_geojson[\"geometry\"].__geo_interface__,\n",
    "                \"type\": \"line\",\n",
    "                \"color\": \"black\",\n",
    "                \"line\": {\"width\": 0.5},\n",
    "            }\n",
    "        ]})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered.drop([\"SECOND CLUSTER\", \"CLUSTER\"], axis=1, inplace=True)\n",
    "data_df_filtered[\"CLUSTER\"] = data_df_filtered[\"SECOND CLUSTER cat\"]\n",
    "data_df_filtered.drop(\"SECOND CLUSTER cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lakshadweep_df = data_df_filtered[data_df_filtered[\"STATE\"] == \"Lakshadweep\"]\n",
    "andaman_nicobar_df = data_df_filtered[data_df_filtered[\"STATE\"] == \"Andaman and Nicobar Islands\"]\n",
    "andaman_df = andaman_nicobar_df[andaman_nicobar_df[\"LATITUDE\"] > 10]\n",
    "nicobar_df = andaman_nicobar_df[andaman_nicobar_df[\"LATITUDE\"] < 10]\n",
    "sikkim_north_bengal = data_df_filtered[data_df_filtered[\"CLUSTER\"] == 2008]\n",
    "himalayas = data_df_filtered[data_df_filtered[\"CLUSTER\"].isin([2005,2030,2002,2013,2023])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andaman and nicobar and lakshadeep need to be seperate clusters, sikkim and north bengal needs to be clustered, himalaya needs to be clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lakshadweep_df[\"CLUSTER\"] = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lat = lakshadweep_df[\"LATITUDE\"].mean()\n",
    "lon = lakshadweep_df[\"LONGITUDE\"].mean()\n",
    "second_clusters_df = second_clusters_df._append({\"LATITUDE\": lat, \"LONGITUDE\": lon, \"CLUSTER\": 4000}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nicobar_df[[\"LATITUDE\", \"LONGITUDE\"]]\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.805, n_samples=10000, n_jobs=-1, random_state=0)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True, n_jobs=-1)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "nicobar_df[\"CLUSTER\"] = labels+5000\n",
    "\n",
    "for i in range(len(cluster_centers)):\n",
    "    second_clusters_df = second_clusters_df._append({\"LATITUDE\": cluster_centers[i][0], \"LONGITUDE\": cluster_centers[i][1], \"CLUSTER\": 5000+i}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = andaman_df[[\"LATITUDE\", \"LONGITUDE\"]]\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.5, n_samples=10000, n_jobs=-1, random_state=0)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True, n_jobs=-1)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "andaman_df[\"CLUSTER\"] = labels+6000\n",
    "\n",
    "for i in range(len(cluster_centers)):\n",
    "    second_clusters_df = second_clusters_df._append({\"LATITUDE\": cluster_centers[i][0], \"LONGITUDE\": cluster_centers[i][1], \"CLUSTER\": 6000+i}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = himalayas[[\"LATITUDE\", \"LONGITUDE\"]]\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.03, n_samples=10000, n_jobs=-1, random_state=0)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True, n_jobs=-1)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "himalayas[\"CLUSTER\"] = labels+7000\n",
    "\n",
    "for i in range(len(cluster_centers)):\n",
    "    second_clusters_df = second_clusters_df._append({\"LATITUDE\": cluster_centers[i][0], \"LONGITUDE\": cluster_centers[i][1], \"CLUSTER\": 7000+i}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sikkim_north_bengal[[\"LATITUDE\", \"LONGITUDE\"]]\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.07, n_samples=10000, n_jobs=-1, random_state=0)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True, n_jobs=-1)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "sikkim_north_bengal[\"CLUSTER\"] = labels+8000\n",
    "\n",
    "for i in range(len(cluster_centers)):\n",
    "    second_clusters_df = second_clusters_df._append({\"LATITUDE\": cluster_centers[i][0], \"LONGITUDE\": cluster_centers[i][1], \"CLUSTER\": 8000+i}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df = pd.concat([andaman_df, nicobar_df, lakshadweep_df, himalayas, sikkim_north_bengal])\n",
    "mapping_df[\"CLUSTER\"] = mapping_df[\"CLUSTER\"].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered.drop(mapping_df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampling_rate = 20000\n",
    "\n",
    "fig = px.scatter_mapbox(mapping_df.sample(sampling_rate, replace=True), lat=\"LATITUDE\", lon=\"LONGITUDE\", color=\"CLUSTER\", zoom=2, opacity=0.8)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\", width=1000, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered[\"CLUSTER\"] = data_df_filtered[\"CLUSTER\"].astype('int64').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered = pd.concat([data_df_filtered, mapping_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered[\"CLUSTER\"] = data_df_filtered[\"CLUSTER\"].astype('int64').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 20000\n",
    "fig = px.scatter_mapbox(data_df_filtered.sample(sampling_rate, replace=True), lat=\"LATITUDE\", lon=\"LONGITUDE\", color=\"CLUSTER\", zoom=2)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\", width=1000, height=1000)\n",
    "fig.update_layout(mapbox={\"layers\":[\n",
    "            {\n",
    "                \"source\": india_geojson[\"geometry\"].__geo_interface__,\n",
    "                \"type\": \"line\",\n",
    "                \"color\": \"black\",\n",
    "                \"line\": {\"width\": 0.5},\n",
    "            }\n",
    "        ]})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_cluster_df = data_df_filtered.groupby(\"OBSERVER ID\").agg({\"CLUSTER\": pd.Series.mode}).reset_index()\n",
    "largest_cluster_df = largest_cluster_df[largest_cluster_df.apply(lambda x: x[\"CLUSTER\"].dtype == \"int64\", axis=1)]\n",
    "largest_cluster_df = largest_cluster_df.rename(columns={\"CLUSTER\": \"LARGEST CLUSTER\"})\n",
    "data_df_filtered = data_df_filtered.merge(largest_cluster_df, on=\"OBSERVER ID\")\n",
    "data_df_filtered[\"LOCAL\"] = data_df_filtered[\"LARGEST CLUSTER\"] == data_df_filtered[\"CLUSTER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observers = data_df_filtered[\"OBSERVER ID\"].unique()\n",
    "observers = pd.Series(observers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered[\"NEXT CLUSTER\"] = data_df_filtered.sort_values(\"DATE TIME\").groupby(\"OBSERVER ID\")[\"CLUSTER\"].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "observer_graphs = {}\n",
    "observers.apply(lambda x: observer_graphs.update({x: nx.DiGraph()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PutInGraph(row):\n",
    "        observer_count = row['NUMLIST']\n",
    "        if (row['CLUSTER'], row['NEXT CLUSTER']) in observer_graphs[row[\"OBSERVER ID\"]].edges():\n",
    "            data = observer_graphs[row[\"OBSERVER ID\"]].get_edge_data(row['CLUSTER'], row['NEXT CLUSTER'])\n",
    "            observer_graphs[row[\"OBSERVER ID\"]].add_edge(row['CLUSTER'], row['NEXT CLUSTER'], weight=data['weight']+1, normal_weight=round((data['weight']+1)/observer_count, 4))\n",
    "        else:\n",
    "            observer_graphs[row[\"OBSERVER ID\"]].add_edge(row['CLUSTER'], row['NEXT CLUSTER'], weight=1, normal_weight=round(1/observer_count, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_df_filtered.dropna().apply(PutInGraph, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "observer_graphs = {k: json_graph.adjacency_data(v) for k, v in observer_graphs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPHING END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observer_cluster_count = data_df_filtered.groupby([\"OBSERVER ID\", \"CLUSTER\"]).size().reset_index(name='COUNT').sort_values(by=\"OBSERVER ID\", ascending=False)\n",
    "observer_cluster_count = observer_cluster_count[observer_cluster_count[\"COUNT\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observer_sequences = data_df_filtered.groupby(\"OBSERVER ID\").agg({\"CLUSTER\": lambda x: list(x), \"NUMLIST\":\"mean\", \"OBSERVATION DATE\": lambda x: list(x)}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered[\"CHECKLIST NUM\"] = data_df_filtered.sort_values(\"OBSERVATION DATE\").groupby(\"OBSERVER ID\").cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_list = []\n",
    "def create_local(row):\n",
    "    id = row[\"OBSERVER ID\"]\n",
    "    clusters = row[\"CLUSTER\"]\n",
    "    window_size = int(round(row[\"NUMLIST\"]/3,0))\n",
    "\n",
    "    local = []\n",
    "    mydict = {}\n",
    "    for i in range(0, len(clusters), window_size):\n",
    "        if i+window_size > len(clusters):\n",
    "            break\n",
    "        if i + 2*window_size > len(clusters):\n",
    "            vals, counts = np.unique(clusters[i:i+2*window_size], return_counts=True)\n",
    "        else: \n",
    "            vals, counts = np.unique(clusters[i:i+window_size], return_counts=True)\n",
    "        mydict[\"CLUSTER\"] = vals[np.argmax(counts)]\n",
    "        mydict[\"FROM\"] = i\n",
    "        mydict[\"TO\"] = i+window_size if i+2*window_size <= len(clusters) else len(clusters)\n",
    "        local.append(mydict.copy())\n",
    "    local_list.append({\"OBSERVER ID\": id, \"LOCAL\": local})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observer_sequences.apply(create_local, axis=1)\n",
    "local_list_df = pd.DataFrame(local_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_list_df_expanded = local_list_df.explode(\"LOCAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_list_df_expanded[\"FROM\"] = local_list_df_expanded[\"LOCAL\"].apply(lambda x: x[\"FROM\"])\n",
    "local_list_df_expanded[\"TO\"] = local_list_df_expanded[\"LOCAL\"].apply(lambda x: x[\"TO\"])\n",
    "local_list_df_expanded[\"CLUSTER\"] = local_list_df_expanded[\"LOCAL\"].apply(lambda x: x[\"CLUSTER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_list_df_expanded.drop(\"LOCAL\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_list_df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add local clusters to data_df_filtered\n",
    "\n",
    "data_df_filtered = data_df_filtered.merge(local_list_df, on=\"OBSERVER ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered_exploded = data_df_filtered.explode(\"LOCAL_y\")\n",
    "data_df_filtered_exploded[\"FROM\"] = data_df_filtered_exploded[\"LOCAL_y\"].apply(lambda x: x[\"FROM\"])\n",
    "data_df_filtered_exploded[\"TO\"] = data_df_filtered_exploded[\"LOCAL_y\"].apply(lambda x: x[\"TO\"])\n",
    "data_df_filtered_exploded[\"LOCAL CLUSTER\"] = data_df_filtered_exploded[\"LOCAL_y\"].apply(lambda x: x[\"CLUSTER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered_exploded_filtered = data_df_filtered_exploded[data_df_filtered_exploded[\"CHECKLIST NUM\"]>=data_df_filtered_exploded[\"FROM\"]]\n",
    "data_df_filtered_exploded_filtered = data_df_filtered_exploded_filtered[data_df_filtered_exploded_filtered[\"CHECKLIST NUM\"]<data_df_filtered_exploded_filtered[\"TO\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered_exploded_filtered.drop([\"LOCAL_y\", \"FROM\", \"TO\"], axis=1, inplace=True)\n",
    "data_df_filtered_exploded_filtered.rename(columns={\"LOCAL_x\": \"NAIVE LOCAL\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered_exploded_filtered[\"LOCAL\"] = data_df_filtered_exploded_filtered[\"CLUSTER\"] == data_df_filtered_exploded_filtered[\"LOCAL CLUSTER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered_exploded_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_geojson = gpd.read_file(\"urban_shape.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "points = gpd.GeoDataFrame(data_df_filtered_exploded_filtered, geometry=gpd.points_from_xy(data_df_filtered_exploded_filtered.LONGITUDE, data_df_filtered_exploded_filtered.LATITUDE))\n",
    "\n",
    "points2 = gpd.sjoin(points, urban_geojson, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points2[\"IS URBAN\"] = points2[\"id\"].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered_exploded_filtered_urban = points2.drop([\"geometry\", \"index_right\",\"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_filtered_exploded_filtered_urban.to_csv(\"data_df_filtered_triple_multi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_clusters_df.to_csv(\"cluster_centers_triple_multi_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(observer_graphs, open(\"observer_graphs_triple.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
